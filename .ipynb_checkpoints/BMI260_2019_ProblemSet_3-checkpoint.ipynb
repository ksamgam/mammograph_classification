{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIOMEDIN 260/RAD260: Problem Set 3 - Mammogram Project\n",
    "\n",
    "## Spring 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name 1:\n",
    "\n",
    "Karina Samuel-Gama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name 2:\n",
    "\n",
    "Areli Valencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Breast cancer has the highest incidence and second highest mortality rate for women in the US.\n",
    "\n",
    "Your task is to utilize machine learning to study mammograms in any way you want (e.g. classification, segmentation) as long as you justify why it is useful to do whatever it is you want to do. Turning in a deep dream assignment using mammograms might be amusing, for example, but not so useful to patients. That being said, choose something that interests you. As the adage goes, \"do what you love, and you’ll never have to work another day in your life, at least in BMI 260.\"\n",
    "\n",
    "Treat this as a mini-project. We highly encourage working with 1 other person, possibly someone in your main project team. \n",
    "\n",
    "In addition to the mammograms themselves, the dataset includes \"ground-truth\" segmentations and `mass_case_description_train_set.csv`, which contains metadata information about mass shapes, mass margins, assessment numbers, pathology diagnoses, and subtlety in the data. Take some time to research what all of these different fields mean and how you might utilize them in your work. You don't need to use all of what is provided to you.\n",
    "\n",
    "Some ideas:\n",
    "\n",
    "1. Use the ROI’s or segmentations to extract features, and then train a classifier based on those features using the algorithms presented to you in the machine learning lectures (doesn't need to use deep learning).\n",
    "\n",
    "2. Use convolutional neural networks. Feel free to use any of the code we went over in class or use your own (custom code, sklearn, keras, Tensorflow etc.). If you dont want to place helper functions and classes into this notebook, place them in a `.py` file in the same folder called `helperfunctions.py` and import them into this notebook.\n",
    "\n",
    "## Data\n",
    "\n",
    "The data is here:\n",
    "\n",
    "https://wiki.cancerimagingarchive.net/display/Public/CBIS-DDSM\n",
    "\n",
    "## Grading and Submission\n",
    "\n",
    "This assignment has 3 components: code, figures (outputs/analyses of your code), and a write-up detailing your mini-project. You will be graded on these categories.\n",
    "\n",
    "If you're OK with Python or R, please place all three parts into this notebook/.Rmd file that we have provided where indicated. We have written template sections for you to follow for simplicity/completeness. When you're done, save as a `.pdf` (please knit to `.pdf` if you are using `.Rmd`, or knit to `.html` and use a browser's \"Print\" function to convert to `.pdf`).\n",
    "\n",
    "If you don't like Python OR R, we will allow you to use a different language, but please turn your assignment in with: 1) a folder with all your code, 2) a folder with all your figures, and 3) a `.tex`/`.doc`/`.pdf` file with a write-up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title: Classification of Benign and Malignant Breast Tumors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Describe what you are doing and why it matters to patients using at least one citation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR WRITTEN ANSWER TO QUESTION 1 HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Describe the relevant statistics of the data. How were the images taken? How were they labeled? What is the class balance and majority classifier accuracy? How will you divide the data into testing, training and validation sets?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPACE FOR CODE FOR QUESTION 2 HERE, SHOULD YOU NEED IT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR WRITTEN ANSWER TO QUESTION 2 HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Describe your data pipeline (how is the data scrubbed, normalized, stored, and fed to the model for training?).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR WRITTEN ANSWER TO QUESTION 3 HERE\n",
    "\n",
    "#Is it ok if we use only the 20GB of data instead of 160GB of data? If so, any suggestions on splitting the data between training, testing, and validation? \n",
    "\n",
    "#What do they mean by scrubbing data? \n",
    "- Everything in the same format (sizes of the images so that they're all the same resolutions) \n",
    "- Might help to normalize the values (GE Machine collects it at pixel distribution compared to other ones). Visually they might look the same. Plt.imshow() might show similar images but if you use the color bar function then the pixel values may be different. There are built in functions to normalize it. Or you can find the mean and standard deviation, and take z-score range. But there's also packages that deal with it. Normalize either (0,1) or (0,255) so long they're comparable values between images. \n",
    "\n",
    "- Convexivity, area from PSET 2 for features to determine the difference between benign and malignant\n",
    "#Features from class: \n",
    "    - Harlick Texture Feature \n",
    "    - Harwaylett Features \n",
    "    \n",
    "\n",
    "#SVM \n",
    "\n",
    "Three sets: training, developing, tests \n",
    "Three sets: training, testing, validation\n",
    "Logistics regression (10 or 20 features) \n",
    "\n",
    "Train it on the training tests -- developing \n",
    "Evaluation AUC curves for the developing sets \n",
    "Am I going to pick model based on AUC or accuracy based on 0.5? Or are we choosing based on specific sensitivity value? \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262, 750, 1599)\n",
      "(113, 750, 1599)\n",
      "(86, 750, 1599)\n",
      "(176, 750, 1599)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image \n",
    "import numpy as np \n",
    "from skimage import io \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Reading in the images from the 20GB of data downloaded \n",
    "scans_path = \"/Users/arelivalencia/Desktop/Images\"\n",
    "list_of_scans = os.listdir(scans_path)\n",
    "\n",
    "num_of_scans = len(list_of_scans)\n",
    "width = 375\n",
    "height = 750\n",
    "\n",
    "allImages = np.zeros((width, height, num_of_scans))\n",
    "\n",
    "for scan_num in range(len(list_of_scans)):\n",
    "    scan_path = os.path.join(scans_path, list_of_scans[scan_num])\n",
    "    image = io.imread(scan_path)\n",
    "    \n",
    "    #There are a variety of interpolation options. I'm not sure which option is best. \n",
    "    resizedImage = cv2.resize(image, dsize = (750,375), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    #In PSET 1, we converted the array type to float32 type then we normalized. The arrays here are uint16. \n",
    "    maxValue = np.amax(resizedImage)\n",
    "    minValue = np.amin(resizedImage)\n",
    "    meanValue = np.mean(resizedImage)\n",
    "    normalized = (resizedImage - minValue)/(maxValue - minValue)\n",
    "    \n",
    "    allImages[:,:,scan_num] = normalized\n",
    "    \n",
    "    #SEGMENTATION? \n",
    "    #Otsu's Threshold?\n",
    "\n",
    "    \n",
    "#Split the array that contains all of the images into training, testing, and validation sets \n",
    "remainingData,training = train_test_split(allImages, test_size=0.3, train_size=0.7)\n",
    "print(remainingData.shape)\n",
    "print(training.shape)\n",
    "\n",
    "testing,validation = train_test_split(remainingData, test_size = 0.67, train_size=0.33)\n",
    "\n",
    "print(testing.shape)\n",
    "print(validation.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Explain how the model you chose works alongside the code for it. Add at least one technical citation to give credit where credit is due.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE FOR QUESTION 4 HERE. USE ADDITIONAL CODE/MARKDOWN CELLS IF NEEDED\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR WRITTEN ANSWER TO QUESTION 4 HERE. USE ADDITIONAL CODE/MARKDOWN CELLS IF NEEDED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. There are many ways to do training. Take us through how you do it (e.g. \"We used early stopping and stopped when validation loss increased twice in a row.\").**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR WRITTEN ANSWER TO QUESTION 5 HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Make a figure displaying your results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE FOR QUESTION 6 HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR WRITTEN ANSWER TO QUESTION 6 HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Discuss pros and cons of your method and what you might have done differently now that you've tried or would try if you had more time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR WRITTEN ANSWER TO QUESTION 7 HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You will not be graded on the performance of your model. You'll only be graded on the scientific soundness of your claims, methodology, evaluation (i.e. fair but insightful statistics), and discussion of the strengths and shortcomings of what you tried. Feel free to reuse some of the code you are/will be using for your projects. The write-up doesn't need to be long (~1 page will suffice), but please cite at least one clinical paper and one technical paper (1 each in questions 1 and 4 at least, and more if needed).**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
